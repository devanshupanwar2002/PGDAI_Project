# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dlr5GWm88vJN9KNGVIkYDAsJxdFOQ2Yz
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

true_df=pd.read_csv('/content/drive/MyDrive/Bhavik_dataset1/true.csv')

true_df

fake_df=pd.read_csv('/content/drive/MyDrive/Bhavik_dataset1/fake.csv')

fake_df

true_df['title']

true_df['text']

import string
import statistics
import pandas as pd
import numpy as np
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt

from pandas import DataFrame
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.layers import TextVectorization, LSTM, Dropout, Bidirectional, Embedding, Dense

TRUE = 0
FAKE = 1
EPOCHS = 5
BATCH_SIZE = 64
COLUMN_TITLE = "title"
COLUMN_TEXT = "text"
COLUMN_IS_FAKE_NEWS= "is_fake_news"
MAX_INPUT_TOKENS = 10000
OUTPUT_SEQUENCE_LENGTH = 400
CLASS_NAMES = ['TRUE', 'FAKE']

fake_df.isnull().sum()

fake_df.drop(['subject','date'],axis=1,inplace=True)
true_df.drop(['subject','date'],axis=1,inplace=True)

fake_df

fake_df['label']=0
true_df['label']=1

fake_df

from sklearn.feature_extraction.text import TfidfVectorizer
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

# Ensure nltk resources are downloaded (for the first time)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def clean_text(text):

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # Remove user @ references and '#' from tweet
    text = re.sub(r'\@\w+|\#','', text)
    # Remove punctuation and numbers
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    # Convert text to lowercase
    text = text.lower()
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]
    return " ".join(lemmatized_text)

fake_df['title']=fake_df['title'].apply(clean_text)

fake_df['text']=fake_df['text'].apply(clean_text)

fake_df

true_df['text']=true_df['text'].apply(clean_text)

true_df['title']=true_df['title'].apply(clean_text)

true_df

fake_df=fake_df.head(21417)

fake_df

from sklearn.utils import shuffle

# Add a 'label' column to each DataFrame
#true_df['label'] = 1  # Assuming 1 represents true news
#fake_df['label'] = 0  # Assuming 0 represents fake news

# Combine the two DataFrames
combined_df = pd.concat([true_df, fake_df], ignore_index=True)

# Shuffle the combined DataFrame to mix true and fake news
df = shuffle(combined_df, random_state=39).reset_index(drop=True)

# Save the combined and labeled DataFrame to a new CSV file
df.to_csv('bhavik_data.csv', index=False)

df

final_df=pd.read_csv('/content/bhavik_data.csv')

final_df

