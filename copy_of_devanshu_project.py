# -*- coding: utf-8 -*-
"""Copy of devanshu project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mSLBoUbQ4l9mrqQPo0dDcjlfOdHAl4vZ

https://www.kaggle.com/datasets/stevenpeutz/misinformation-fake-news-text-dataset-79k
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Read the first 100 rows of the CSV file
true_df = pd.read_csv('/content/drive/MyDrive/dataset/DataSet_Misinfo_TRUE.csv', nrows=1000)
fake_df= pd.read_csv('/content/drive/MyDrive/dataset/DataSet_Misinfo_FAKE.csv', nrows=1000)

import pandas as pd
from sklearn.utils import shuffle

# Load the datasets
true_data_path = 'path/to/your/true_data.csv'
fake_data_path = 'path/to/your/fake_data.csv'

#true_df = pd.read_csv('/content/DataSet_Misinfo_TRUE.csv', nrows=1000)
#fake_df = pd.read_csv('/content/DataSet_Misinfo_FAKE.csv', nrows=1000)

# Add a 'label' column to each DataFrame
true_df['label'] = 1  # Assuming 1 represents true news
fake_df['label'] = 0  # Assuming 0 represents fake news

# Combine the two DataFrames
combined_df = pd.concat([true_df, fake_df], ignore_index=True)

# Shuffle the combined DataFrame to mix true and fake news
df = shuffle(combined_df, random_state=42).reset_index(drop=True)

# Save the combined and labeled DataFrame to a new CSV file
combined_df.to_csv('combined_data.csv', index=False)

df

df = df.drop(columns=['Unnamed: 0'])

df.columns

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.utils import to_categorical

df1=pd.read_csv('dataset.csv')

# # Basic preprocessing
# texts = df['text'].astype(str)
# labels = df['label']

# Basic preprocessing
texts = df1['text'].astype(str)
labels = df1['label']

# Encode labels (if they are not already in binary form)
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)
categorical_labels = to_categorical(encoded_labels)

# Split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(texts, categorical_labels, test_size=0.2, random_state=42)

# Tokenize text
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences to ensure uniform input size
max_seq_length = max([len(seq) for seq in X_train_seq])
X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length)

# Define model architecture
model = Sequential([
    Embedding(input_dim=5000, output_dim=100, input_length=max_seq_length),
    LSTM(100),
    Dense(2, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
model.fit(X_train_pad, y_train, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test))

import numpy as np

# Make predictions on the test set
predictions = model.predict(X_test_pad)

# The predictions will be in the form of class probabilities. You need to convert these to class labels (0 or 1 in this case).
predicted_labels = np.argmax(predictions, axis=1)

# If you want to convert these back to the original label names (fake or real), use the LabelEncoder you fitted earlier:
predicted_label_names = label_encoder.inverse_transform(predicted_labels)

# Example: printing the first 10 predictions
for i in range(10):
    print(f"News: {X_test.iloc[i][:50]}... -> Predicted: {'Real' if predicted_label_names[i] == 1 else 'Fake'}")

!pip install easyocr

import easyocr

# Initialize the EasyOCR reader
reader = easyocr.Reader(['en'])  # Specify the languages you want to recognize (e.g., English)

# Load the image
image_path = '/content/WhatsApp Image 2024-02-14 at 8.09.10 PM.jpeg'
result = reader.readtext(image_path)
#print(result)
text=[]
# Print the extracted text
for detection in result:
    print(detection[1])  # The detected text
    text.append(detection[1])
result = ' '.join(text)

result

# Convert the new text to a sequence
new_text_seq = tokenizer.texts_to_sequences([result])

# Pad the sequence
new_text_pad = pad_sequences(new_text_seq, maxlen=max_seq_length)

# Predict the class using the trained model
new_prediction = model.predict(new_text_pad)

# Convert the prediction probability to class label
new_predicted_label = np.argmax(new_prediction, axis=1)
new_predicted_label_name = label_encoder.inverse_transform(new_predicted_label)

# Print the prediction
new_predicted_label_name
print(new_predicted_label)

model.save('/content/drive//MyDrive/My_model')

import pickle

# Saving the tokenizer
tokenizer_path = '/content/drive/MyDrive/tokenizer.pickle'  # Specify your path here
with open(tokenizer_path, 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

!pip install -q streamlit

print(max_seq_length)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# # streamlit_app.py
# import os
# from PIL import Image
# 
# import streamlit as st
# from tensorflow.keras.models import load_model
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# import numpy as np
# import easyocr
# import pickle
# 
# 
# # Load your trained model and tokenizer
# model = load_model('/content/drive/MyDrive/My_model')
# with open('/content/drive/MyDrive/tokenizer.pickle', 'rb') as handle:
#     tokenizer = pickle.load(handle)
# 
# 
# max_seq_length = 1138  # Update this to the max sequence length from your training
# 
# # Initialize EasyOCR reader
# reader = easyocr.Reader(['en'])  # Ensure you've downloaded the necessary language models
# 
# def predict_news(text):
#     sequence = tokenizer.texts_to_sequences([text])
#     pad = pad_sequences(sequence, maxlen=max_seq_length)
#     prediction = model.predict(pad)
#     label = np.argmax(prediction, axis=1)
#     return 'Real' if label == 1 else 'Fake'
# 
# def extract_text_from_image(image):
#     detected_text = reader.readtext(np.array(image), paragraph=True)
#     extracted_text = " ".join([text[1] for text in detected_text])
#     return extracted_text
# def save_uploaded_file(directory, uploaded_file, save_as):
#     # Create the directory if it doesn't exist
#     if not os.path.exists(directory):
#         os.makedirs(directory)
#     # Save the uploaded file to the specified directory with the specified name
#     file_path = os.path.join(directory, save_as)
#     with open(file_path, "wb") as f:
#         f.write(uploaded_file.getbuffer())
#     return file_path
# 
# 
# # Streamlit UI
# st.title('Fake News Detection App')
# st.write("Upload an image containing text or enter text manually to check if it's real or fake news.")
# 
# # Image Upload
# uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
# if uploaded_file is not None:
#      # Specify the directory and the filename you want to save the uploaded file as
#     directory = "uploaded_images"
#     save_as = "uploaded_image.jpg"  # Specify the name you want to save the file as
# 
#     # Save the uploaded image
#     saved_path = save_uploaded_file(directory, uploaded_file, save_as)
# 
#     if saved_path:  # If the image is saved successfully
#         st.success(f"Image successfully saved at: {saved_path}")
#         # Display the saved image
#         image = Image.open(saved_path)
# 
#     #user_input = extract_text_from_image(uploaded_file)
#     result = reader.readtext(saved_path)
#     #print(result)
#     text=[]
#     # Print the extracted text
#     for detection in result:
#         print(detection[1])  # The detected text
#         text.append(detection[1])
#     user_input = ' '.join(text)
#     st.write("Extracted Text:")
#     st.write(user_input)
# else:
#     user_input = st.text_area("Or Enter Text Here", "")
# 
# if st.button('Predict'):
#     if user_input:
#         prediction = predict_news(user_input)
#         st.write(f'Prediction: *{prediction}*')
#     else:
#         st.write("Please upload an image or enter text to predict.")

!streamlit run app.py &>/content/logs.txt &

import urllib
print("Password/Enpoint IP for localtunnel is:",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))

!npx localtunnel --port 8501